{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Data Science (for complete beginners) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. What is Data Science?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Depends who you ask, but my definition is):\n",
    "\t*Data Science is a multidisciplinary field that combines Software Engineering, Statistics and good old fashioned domain knowledge.\n",
    "\t* Data Scientists are people who are better at statistics than any software engineer and better at software engineering than any statistician\n",
    "\n",
    "* What's the difference between Automation and Data Science?\n",
    "\t* Automation is about creating an agent who is able to perform a task, Data Science is about providing new insight into problem domains\n",
    "\n",
    "* Okay, so what is Big Data then?\n",
    "\t* Good question ...\n",
    "\t* Big Data is phrase that has almost become meaningless, what makes data big?\n",
    "\t* My opinion, if your dataset doesn't fit in the main memory of your machine, your data is big.\n",
    "\t* Worth noting, bigger doesn't always mean better. More important to get a represntative sample\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data Science lifecycle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Where do I get my data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That depends entirely on what kind of data you wish to work with, and what you're trying to achieve. If you wish to experiment with some of the techniques covered today, or want to play with some new toy from Google (TensorFlow anyone?) then pubically available datasets are a great chance to get to grips with some of these tools. \n",
    "\n",
    "However if you wish to deliver some new insight for you or the business, then you'll need to find/build your own dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Public Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many great resources online with collections of publically available datasets, although some of my personal favourites include: \n",
    "\n",
    "* [Kaggle (the home for all things Data Science)](https://www.kaggle.com/datasets)\n",
    "* [University of California, Irvine Machine Learning repository](http://archive.ics.uci.edu/ml/)\n",
    "* [Quandl](https://www.quandl.com)\n",
    "* [Amazon's AWS Datasets](http://aws.amazon.com/datasets/)\n",
    "* [University of Edinburgh's dataset collection](http://www.inf.ed.ac.uk/teaching/courses/dme/html/datasets0405.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Understanding your data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you begin modelling your data there is really two important questions\n",
    "\n",
    "* What kind of task do I want this data to perform with this data (Regression vs Classification vs Clustering)\n",
    "* What kind of learning do I need to in order to achieve that task (Supervised vs Unsupervised)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised vs Unsupervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When discussing learning methods (we want our algorithms to learn features of our data, in order to create models) we often mention what type of learning we wish to do, these are  categorised into *supervised* and *unsupervised* learning tasks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Supervised"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supervised learning is the process of learning by example, similar to a teacher teaching a student. Your training data consists of training examples where we have some feature/predictor variables and a label/target variable.\n",
    "\n",
    "$$\\mathbf{x} = (x_1,x_2,...,x_n)^T$$\n",
    "$$f(\\mathbf{x}) \\rightarrow y  $$\n",
    "\n",
    "An example of a supervised task would be to determine the credit worthiness, using features obtained from their financial history, and we have similar data of customers of customer who may have defaulted in the past. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unsupervised"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsupervised learning is the opposite, our data is unlabeled in this case, and our algorithms try to find some structure or rules based on the data we provide. \n",
    "\n",
    "One of the most famous examples of an unsupervised machine learning algorithm in the real world would probably be Googles PageRank algorithm. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression vs Classification vs Clustering "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "\n",
    "* Classification is probably the most intuitive task, your algorithm will attempt to assign your observations into discrete classes, based on previously observed examples.\n",
    "\n",
    "* An algorithm that implements classifiation is called a **classifier**, and are instances of a supervised learning task.\n",
    "\n",
    "* i.e. We are classifying based on example classes we've seen before, where we have some label determining the correct result. \n",
    "\n",
    "Typical classification tasks include:\n",
    "\n",
    "* Spam or Ham detection for emails\n",
    "* Credit worthyness, would you give this person a loan?\n",
    "* Detecting whether or not a human face is present in the picture\n",
    "\n",
    "* To the board for an Oranges to Lemons example!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression\n",
    "\n",
    "* Where classifiers deal with discrete classifications, regression algorithms deal with continuous response variables. \n",
    "* Remember $y=mx + c$ ? Then you've used a regression algorithm!\n",
    "* Regression models aim to predict a response variable, given some known variables. \n",
    "* When you're predicting within a range already observed in the dataset, you are *interpolating*\n",
    "* When you're predicting outside of the range of the dataset, you are *extrapolating*\n",
    "\n",
    "Typical regression tasks include:\n",
    "\n",
    "* Predicting the price of the stock market \n",
    "* Calculating life expectancy\n",
    "* I'm sure people in the room have even more examples...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "\n",
    "* Unlike Classification and Regression, Clustering is an unsupervised task\n",
    "* Wish to group similar or related data into some sort of cluster\n",
    "* This can be a hard task to get right, and can be used to achieve different goals\n",
    "* Generally used where getting hold of labelled data is impossible or infeasable! \n",
    "\n",
    "Typical examples include:\n",
    "\n",
    "* Topic modeling, grouping articles together based on underlying \"themes\" of your articles\n",
    "* Forming connected graphs of references between documents to infer relevancy (ever heard of Google?)\n",
    "* Analysis of 'tribe' behaviour in social groups\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whenever we have a new dataset to work with, it's important to be able to manipulate and play with our new dataset. This helps us get a feel for our dataset, and allows us to:\n",
    "\n",
    "* Maximise the insight into the dataset and it's main characteristics \n",
    "* Detect mistakes, missing values, outliers and anomalies\n",
    "* Determine relationships between the input variables\n",
    "* Improve the quality of our data and avoid feeding our models with garbage\n",
    "* Determine what algorithms we wish to use to build our models. \n",
    "\n",
    "So with this in mind ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. How do I preprocess my data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When creating statistical models, our models are only as good as the data we provide. This experience is nicely summarised by the expression \"Garbage in, Garbage out\".\n",
    "\n",
    "Therefore it's worth investing time upfront to ensure your data is up to par, saving you headaches (and poor results) in the future. For a Data Scientist, this preprocessing takes up the majority of your time.\n",
    "\n",
    "As a result, there are some common problems that you need to look out for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Balanced Datasets\n",
    "\n",
    "Imagine you were building a classifier to determine whether a Physicist, who has recently been awarded their PhD, was likely to be awarded a nobel prize in the future. What simple rule would guarantee excellent performance of your classifier?\n",
    "\n",
    "This is why having balanced datasets is important as they can skew the percieved performance of your classifier. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling\n",
    "\n",
    "Below is a table containing the first 5 records from a collection, and we're looking at two columns in particular. Can you spot what may be an issue here?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11</td>\n",
       "      <td>34</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25</td>\n",
       "      <td>67</td>\n",
       "      <td>0.9968</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15</td>\n",
       "      <td>54</td>\n",
       "      <td>0.9970</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.2</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17</td>\n",
       "      <td>60</td>\n",
       "      <td>0.9980</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11</td>\n",
       "      <td>34</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "output_type": "execute_result",
     "metadata": {}
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "wineQ = pd.io.parsers.read_csv(\n",
    "    'https://raw.githubusercontent.com/schafer14/Machine-Learning-Wine-DataSets/master/files/winelist.csv',\n",
    "    )\n",
    "header = wineQ.columns.values\n",
    "wineQ.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Building your models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A combination of task + learning method generally drives the choice of our models. \n",
    "\n",
    "Today we will discuss three different models, k-Nearest Neighbour, Decision Trees, and their bigger brother, Random Forests.\n",
    "\n",
    "These models were chosen today because they are relatively simple to understand, quick to explain, and the resulting model is human interpretable (i.e. we understand why a model made a choice\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbour"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "k-Neartest Neighbour (or k-NN for short) is a classifier that tries to to determine the label of the input, based on its relative distance in some multidimensional feature space. \n",
    "\n",
    "Essentially you're trying to find similar examples, and assign classes based on similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-743c88ae60bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;31m# Plot the decision boundary. For that, we will assign a color to each\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;31m# point in the mesh [x_min, m_max]x[y_min, y_max].\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mx_min\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0my_min\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
      "\u001b[0;32m/Users/angusscott/anaconda2/lib/python2.7/site-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1967\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1968\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1969\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1970\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1971\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/angusscott/anaconda2/lib/python2.7/site-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36m_getitem_column\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1974\u001b[0m         \u001b[0;31m# get column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1975\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1976\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_item_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1977\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1978\u001b[0m         \u001b[0;31m# duplicate columns & possible reduce dimensionality\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/angusscott/anaconda2/lib/python2.7/site-packages/pandas/core/generic.pyc\u001b[0m in \u001b[0;36m_get_item_cache\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m   1087\u001b[0m         \u001b[0;34m\"\"\" return the cached item, item represents a label indexer \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1088\u001b[0m         \u001b[0mcache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_item_cache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1089\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1090\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1091\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn import neighbors, datasets\n",
    "\n",
    "n_neighbors = 15\n",
    "\n",
    "# import some data to play with\n",
    "wineQ = pd.io.parsers.read_csv(\n",
    "    'https://raw.githubusercontent.com/schafer14/Machine-Learning-Wine-DataSets/master/files/winelist.csv',\n",
    "    )\n",
    "X = wineQ.drop('quality', axis=1)  # we only take the first two features. We could\n",
    "                      # avoid this ugly slicing by using a two-dim dataset\n",
    "y = wineQ['quality']\n",
    "\n",
    "h = .02  # step size in the mesh\n",
    "\n",
    "# Create color maps\n",
    "cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])\n",
    "cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])\n",
    "\n",
    "for weights in ['uniform']:\n",
    "    # we create an instance of Neighbours Classifier and fit the data.\n",
    "    clf = neighbors.KNeighborsClassifier(n_neighbors, weights=weights)\n",
    "    clf.fit(X, y)\n",
    "\n",
    "    # Plot the decision boundary. For that, we will assign a color to each\n",
    "    # point in the mesh [x_min, m_max]x[y_min, y_max].\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.figure()\n",
    "    plt.pcolormesh(xx, yy, Z)\n",
    "\n",
    "    # Plot also the training points\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y)\n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())\n",
    "    plt.title(\"3-Class classification (k = %i, weights = '%s')\"\n",
    "              % (n_neighbors, weights))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Trees (and Random Forests)\n",
    "\n",
    "Imgine the game 20 Questions, where you try and and guess the name of a celebrity by asking  a series of questions about them. \n",
    "Decision tress are a very similar concept, by maximising a function that measures *information gain* you can create a classifier that asks a series of questions about data to try and discern the class of that data point. \n",
    "\n",
    "Decision Trees are not particularly performant models, and are actually known as weak leaners. But imagine, could you build lots of these things, which all ask slightly different questions, and through some wisdom of the crowd come to better conclusions? Yes we can! This is called an *ensamble method*, and it allows you to combine a collection of weak learners, and create a strong learner. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluating your models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have your models you need some method of scoring or measuring their performance relative to one another. \n",
    "\n",
    "It's bad practice to test your model on the same data your trained it with. So we will generally take our dataset and split it into a testing component, and a training compoent. 30/70 is generally considered to be a good split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision, Recall and Accuracy\n",
    "\n",
    "When evaluating models, accuracy is the most intuitive measure. Surely if I am correct more often, then my classifier is great?\n",
    "\n",
    "Because this isn't strictly true, we have two other measures, *Precision* and *Recall*\n",
    "\n",
    "Precision is the ability of a classifier not to miclassify a posiitve as a negative\n",
    "\n",
    "Recall is the ability of the classifier to find all posiitve examples \n",
    "\n",
    "$precision = \\frac{tp}{tp + fp}$\n",
    "\n",
    "$recall = \\frac{tp}{tp + fn}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Conclusions and further reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today we have covered:\n",
    "\n",
    "* What Data Science is, and took our first step to becoming world leading Data Scientists\n",
    "* Looked at some popular online data repositories\n",
    "* Discussed the differences between supervised and unsupervised learning\n",
    "* Considered the differences between classification, regression and clustering tasks\n",
    "* Discovered why it's so important to look at our data before we even start to process or model it\n",
    "* The idea of Garbage in, Garbage out\n",
    "* Cleaning and scaling our data, in order to improve the performance of our models\n",
    "* Learnt about three popular models for machine learning: \n",
    "\t* k-Nearest Neighbours\n",
    "\t* Decision Trees\n",
    "\t* Random Forests\n",
    "* Discussed how we should evaluate our models, and why evaluation is so important in the first place. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further watching/listening/reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Watch:\n",
    "\n",
    "Andrew Ng's Stanford course on Machine Learning\n",
    "\n",
    "Listen:\n",
    "\n",
    "Talking Machines\n",
    "Goldmans Sachs episode on Data Science\n",
    "Data Viz\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}